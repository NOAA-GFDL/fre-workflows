#!/usr/bin/env python3
"""
rename-split-to-pp: Create per-variable timeseries from shards.

This script links each input file into a canonical directory structure,
renaming files based on their content and metadata.

Environment variables required:
    inputDir: Input directory containing split netCDF files
    outputDir: Output directory for reorganized files
    component: The history source/component name (e.g., 'atmos_daily')
    use_subdirs: 'True' or 'False' - whether to use subdirectories (for regridding case)
"""

import os
import sys
import re
import glob
import subprocess
from pathlib import Path

# Import shared utilities
script_dir = Path(__file__).resolve().parent
sys.path.insert(0, str(script_dir.parent / 'shared'))
from shared import (
    err,
    get_freq_and_format_from_two_dates,
    get_freq_and_format_from_two_days,
    get_freq_and_format_from_hours
)


def run_command(cmd: list, capture_output: bool = True) -> subprocess.CompletedProcess:
    """Run a shell command and return the result."""
    return subprocess.run(cmd, capture_output=capture_output, text=True)


def cdo_sinfo(filepath: str) -> bool:
    """Check if CDO can open the file."""
    result = run_command(['cdo', 'sinfo', filepath])
    return result.returncode == 0


def cdo_ntime(filepath: str) -> int:
    """Get the number of timesteps in a file using CDO."""
    result = run_command(['cdo', 'ntime', filepath])
    if result.returncode == 0:
        return int(result.stdout.strip())
    return 0


def cdo_showtimestamp(filepath: str) -> list:
    """Get timestamps from a file using CDO."""
    result = run_command(['cdo', 'showtimestamp', filepath])
    if result.returncode == 0:
        return result.stdout.split()
    return []


def ncdump_header(filepath: str) -> str:
    """Get the header of a netCDF file using ncdump."""
    result = run_command(['ncdump', '-h', filepath])
    if result.returncode == 0:
        return result.stdout
    return ''


def ncdump_time_bnds(filepath: str) -> str:
    """Get time_bnds from a netCDF file using ncdump."""
    result = run_command(['ncdump', '-t', '-v', 'time_bnds', filepath])
    if result.returncode == 0:
        return result.stdout
    return ''


def has_time_dimension(header: str, varname: str) -> bool:
    """Check if a variable has a time dimension."""
    # Look for patterns like "var(time, ...)" or "var(time)"
    pattern = rf'\b{re.escape(varname)}\s*\([^)]*\btime\b'
    return bool(re.search(pattern, header))


def isodatetime(date_str: str, offset: str = None, max_count: int = None,
                print_format: str = None) -> str:
    """
    Call the isodatetime utility (cylc utility).

    Args:
        date_str: The date string to process
        offset: Optional offset to apply
        max_count: Maximum count for repeat sequences
        print_format: Output format for the date

    Returns:
        The processed date string
    """
    cmd = ['isodatetime']

    if max_count is not None:
        cmd.extend(['--max', str(max_count)])

    if print_format is not None:
        cmd.extend(['--print-format', print_format])

    cmd.append(date_str)

    if offset is not None:
        cmd.extend(['--offset', offset])

    result = run_command(cmd)
    if result.returncode == 0:
        return result.stdout.strip()
    return ''


def isodatetime_repeat(start_date: str, freq: str, max_count: int) -> list:
    """
    Generate a sequence of dates using isodatetime.

    Args:
        start_date: Start date
        freq: Frequency/duration
        max_count: Maximum count

    Returns:
        List of date strings
    """
    full_repeat = f"R{max_count}/{start_date}/{freq}"
    cmd = ['isodatetime', '--max', str(max_count), full_repeat]
    result = run_command(cmd)
    if result.returncode == 0:
        return result.stdout.strip().split('\n')
    return []


def isodatetime_duration(date1: str, date2: str) -> str:
    """
    Calculate the duration between two dates using isodatetime.

    Args:
        date1: First date
        date2: Second date

    Returns:
        Duration string in ISO8601 format
    """
    cmd = ['isodatetime', date1, date2]
    result = run_command(cmd)
    if result.returncode == 0:
        return result.stdout.strip()
    return ''


def parse_time_bnds(ncdump_output: str) -> tuple:
    """
    Parse time_bnds output from ncdump to get the first two dates.

    Args:
        ncdump_output: Output from ncdump -t -v time_bnds

    Returns:
        Tuple of (date1, date2) strings
    """
    # Get the last two lines which contain the data
    lines = ncdump_output.strip().split('\n')

    # Look for the data section
    for i, line in enumerate(lines):
        if 'time_bnds =' in line:
            # Get the next lines with actual data
            data_lines = lines[i + 1:]
            break
    else:
        data_lines = lines[-2:]

    # Join and parse
    data_text = ' '.join(data_lines)
    # Remove quotes, semicolons, and extra whitespace
    data_text = data_text.replace('"', '').replace(';', '').strip()

    # Split by comma to get date pairs
    parts = [p.strip() for p in data_text.split(',')]

    if len(parts) >= 2:
        d1 = parts[0].replace(' ', 'T')
        d2 = parts[1].replace(' ', 'T')
        return (d1, d2)

    return (None, None)


def promote_chunk_days_to_duration(chunk: str) -> str:
    """
    Adjust chunk duration for edge cases.

    1. If duration is 365 or 366 days, count it as a year
    2. If duration is ~30 days, count it as a month
    3. If duration is 180-185 days, count it as 6 months

    Args:
        chunk: Duration string like 'P365D'

    Returns:
        Adjusted duration string
    """
    match = re.match(r'^P(\d+)D$', chunk)
    if not match:
        return chunk

    days = int(match.group(1))

    # Monthly (27-31 days)
    if 27 < days < 32:
        print(f"NOTE: Promoting {chunk} to P1M")
        return 'P1M'

    # 6 months (180-185 days)
    if 179 < days < 186:
        print(f"NOTE: Promoting {chunk} to P6M")
        return 'P6M'

    # Years (within 0.3% of 365 days)
    years_int = days // 365
    years_frac = days / 365 - years_int
    if years_frac < 0.003 and years_int > 0:
        print(f"NOTE: Promoting {chunk} to P{years_int}Y")
        return f'P{years_int}Y'

    return chunk


def run_ppval(filepath: str) -> None:
    """
    Run fre pp ppval on a file to validate timesteps.

    Args:
        filepath: Path to the file to validate
    """
    cmd = ['fre', '-v', 'pp', 'ppval', '--path', filepath]
    result = run_command(cmd, capture_output=False)
    if result.returncode != 0:
        err(f"WARNING: ppval returned non-zero for {filepath}")


def process_file(filepath: str, output_dir: str,
                 use_subdirs: bool, subdir: str = None) -> bool:
    """
    Process a single input file.

    Args:
        filepath: Full path to the input file
        output_dir: Base output directory
        use_subdirs: Whether to use subdirectories
        subdir: Subdirectory name (if use_subdirs is True)

    Returns:
        True if file was processed successfully, False otherwise
    """
    filename = os.path.basename(filepath)
    print(f"Processing: {filename}")

    # Parse filename: DATE.label.VAR.nc or DATE.label.tile.VAR.nc
    parts = filename.split('.')
    dot_count = len(parts) - 1  # Number of dots

    if dot_count == 4:
        # Tiled: DATE.label.tile.VAR.nc
        date = parts[0]
        label = parts[1]
        tile = parts[2]
        var = parts[3]
    elif dot_count == 3:
        # Non-tiled: DATE.label.VAR.nc
        date = parts[0]
        label = parts[1]
        var = parts[2]
        tile = ""
    else:
        err(f"WARNING: Unexpected filename format: {filename}")
        return False

    # Skip average_DT, average_T1, average_T2 variables
    if re.match(r'average_..', var):
        return False

    # Check if CDO can open the file
    if not cdo_sinfo(filepath):
        err(f"WARNING: Skipping file that CDO cannot open: {filepath}")
        return False

    # Check for time dimension (static vs timeseries)
    header = ncdump_header(filepath)
    is_static = not has_time_dimension(header, var)

    if is_static:
        timesteps = 1
    else:
        timesteps = cdo_ntime(filepath)

    # Determine frequency and format
    if timesteps == 1:
        if is_static:
            freq = 'P0Y'
            fmt = 'CCYY'
        else:
            # Timeseries with one timestep - need to use time_bnds
            time_bnds_output = ncdump_time_bnds(filepath)
            if not time_bnds_output:
                err(f"WARNING: Skipping timeseries file with one timestep but no time_bnds: {filepath}")
                return False

            d1, d2 = parse_time_bnds(time_bnds_output)
            if d1 is None or d2 is None:
                err(f"WARNING: Could not parse time_bnds: {filepath}")
                return False

            # Check if dates are numeric (days) or ISO format
            if re.match(r'^[\d.]+$', d1):
                freq, fmt = get_freq_and_format_from_two_days(float(d1), float(d2))
            else:
                freq, fmt = get_freq_and_format_from_two_dates(d1, d2)

            if freq == 'error':
                return False
    else:
        # Multiple timesteps - estimate frequency from first two timestamps
        timestamps = cdo_showtimestamp(filepath)
        if len(timestamps) < 2:
            err(f"WARNING: Could not get timestamps from {filepath}")
            return False

        d1 = timestamps[0]
        d2 = timestamps[1]

        if d1 == "0000-00-00T00:00:00":
            err(f"WARNING: Skipping file with t=0 timestamp of zeros: {filepath}")
            return False

        freq, fmt = get_freq_and_format_from_two_dates(d1, d2)
        if freq == 'error':
            return False

    # Calculate date2 from # timesteps, date1, and frequency
    date1 = isodatetime(date)
    if not date1:
        err(f"WARNING: Could not parse date: {date}")
        return False

    # Generate sequence to get date2
    dates = isodatetime_repeat(date1, freq, timesteps)
    if not dates:
        err(f"WARNING: Could not generate date sequence for {filepath}")
        return False

    date2 = dates[-1] if dates else date1

    # Calculate chunk (duration of the file)
    date2_with_offset = isodatetime(date2, offset=freq)
    chunk = isodatetime_duration(date1, date2_with_offset)

    # Promote days to more canonical durations
    chunk = promote_chunk_days_to_duration(chunk)

    # Create new filename
    if freq == 'P0Y':
        # Static file
        if tile:
            newfile = f"{label}.{var}.{tile}.nc"
        else:
            newfile = f"{label}.{var}.nc"
    else:
        # Timeseries file
        # Format dates for filename
        date1_str = isodatetime(date1, print_format=fmt)
        date2_str = isodatetime(date2, print_format=fmt)

        if date1_str and date2_str:
            date1_str = date1_str.replace('T', '')
            date2_str = date2_str.replace('T', '')

            if tile:
                newfile = f"{label}.{date1_str}-{date2_str}.{var}.{tile}.nc"
            else:
                newfile = f"{label}.{date1_str}-{date2_str}.{var}.nc"
        else:
            err(f"WARNING: Could not format dates for {filepath}")
            return False

    # Determine output directory
    if use_subdirs and subdir:
        out_path = os.path.join(output_dir, subdir, label, freq, chunk)
    else:
        out_path = os.path.join(output_dir, label, freq, chunk)

    # Create output directory
    os.makedirs(out_path, exist_ok=True)

    # Check if output file already exists
    out_file = os.path.join(out_path, newfile)
    if os.path.exists(out_file):
        err(f"Output location {out_file} already exists, not overwriting")
        return True

    # Link the file(s)
    if tile:
        # For tiled files, link all 6 tiles
        # The tile name is in the 3rd position of the filename: DATE.label.tileN.VAR.nc
        tile_patterns = ['tile1', 'tile2', 'tile3', 'tile4', 'tile5', 'tile6']
        src_filename = os.path.basename(filepath)
        dst_filename = os.path.basename(out_file)
        src_dir = os.path.dirname(filepath)
        dst_dir = os.path.dirname(out_file)

        for t in tile_patterns:
            # Replace only the tile component in the filename, not in the path
            src_fn = src_filename.replace('.tile1.', f'.{t}.')
            dst_fn = dst_filename.replace('.tile1.', f'.{t}.')
            src_file = os.path.join(src_dir, src_fn)
            dst_file = os.path.join(dst_dir, dst_fn)

            if os.path.exists(src_file):
                os.link(src_file, dst_file)
                if freq != 'P0Y':
                    run_ppval(dst_file)
    else:
        os.link(filepath, out_file)
        if freq != 'P0Y':
            run_ppval(out_file)

    return True


def find_matching_files(directory: str, component: str) -> list:
    """
    Find files matching the expected pattern for the given component.

    Args:
        directory: Directory to search in
        component: Component name (e.g., 'atmos_daily')

    Returns:
        List of matching file paths
    """
    # Pattern: DATE.component(.tile1).VAR.nc
    # Only look for tile1 files for tiled data (will process all tiles together)
    pattern1 = os.path.join(directory, f"*.{component}.*.nc")
    pattern2 = os.path.join(directory, f"*.{component}.tile1.*.nc")

    # Regex to detect tile patterns in filename only (not path)
    tile_pattern = re.compile(r'\.tile[2-6]\.')

    files = set()
    for f in glob.glob(pattern1):
        filename = os.path.basename(f)
        # Skip if this is a tiled file but not tile1 (check filename only)
        if tile_pattern.search(filename):
            continue
        files.add(f)

    for f in glob.glob(pattern2):
        files.add(f)

    return sorted(files)


def main():
    """Main entry point for rename-split-to-pp."""
    # Get environment variables
    input_dir = os.environ.get('inputDir')
    output_dir = os.environ.get('outputDir')
    component = os.environ.get('component')
    use_subdirs_str = os.environ.get('use_subdirs', 'False')

    # Validate required environment variables
    if not input_dir:
        err("Error: inputDir environment variable is not set")
        sys.exit(1)
    if not output_dir:
        err("Error: outputDir environment variable is not set")
        sys.exit(1)
    if not component:
        err("Error: component environment variable is not set")
        sys.exit(1)

    use_subdirs = use_subdirs_str.lower() == 'true'

    # Print arguments
    print("Arguments:")
    print(f"    input dir: {input_dir}")
    print(f"    output dir: {output_dir}")
    print(f"    component: {component}")
    print(f"    use subdirs: {use_subdirs_str}")

    # Print utilities
    print("Utilities:")
    for tool in ['cdo', 'ncdump', 'ncks', 'isodatetime']:
        result = run_command(['which', tool])
        if result.returncode == 0:
            print(f"    {tool}: {result.stdout.strip()}")
        else:
            err(f"WARNING: {tool} not found")

    # Verify input directory exists
    if not os.path.isdir(input_dir):
        err(f"Error: Input directory '{input_dir}' does not exist or isn't a directory")
        sys.exit(1)

    # Verify output directory exists
    if not os.path.isdir(output_dir):
        err(f"Error: Output directory '{output_dir}' does not exist or isn't a directory")
        sys.exit(1)

    files_processed_total = 0

    if use_subdirs:
        print("using subdirs")
        # Process each subdirectory
        for subdir in os.listdir(input_dir):
            subdir_path = os.path.join(input_dir, subdir)
            if not os.path.isdir(subdir_path):
                continue

            files = find_matching_files(subdir_path, component)
            if not files:
                err(f"No input files in {subdir_path}")
                continue

            print(f"Processing subdirectory: {subdir}")
            for filepath in files:
                print(filepath)
                if process_file(filepath, output_dir,
                                use_subdirs=True, subdir=subdir):
                    files_processed_total += 1

    else:
        print("not using subdirs")
        files = find_matching_files(input_dir, component)

        if not files:
            err("ERROR: No input files found")
            sys.exit(1)

        print(' '.join([os.path.basename(f) for f in files]))
        for filepath in files:
            print(filepath)
            if process_file(filepath, output_dir,
                            use_subdirs=False):
                files_processed_total += 1

    print(f"{files_processed_total} files processed")

    if files_processed_total == 0:
        err("Error in rename-split-to-pp: no files in input dir matched requirements for processing!")
        sys.exit(1)

    print("Natural end of the shard renaming")
    sys.exit(0)


if __name__ == '__main__':
    main()
