#!/bin/bash
set -euo pipefail
set -x

#
# Create per-variable timeseries from shards
#

# Create timeseries for a given source directory location
function process_timeseries {
    # Loop over the leaf directories which contain the datetime info
    dirs=$(find . -mindepth 2 -type d)
    if [[ -z $dirs ]]; then
        echo ERROR: No input directories found!
        exit 1
    fi

    for dir in $dirs; do
        freq=$(echo $dir | cut -d/ -f2)
        chunk=$(echo $dir | cut -d/ -f3)

        # Need a better way to compare durations, i.e. P1Y equals P12M
        if [[ $inputChunk == P12M ]]; then
            inputChunk=P1Y
        fi

        if [[ $chunk != $inputChunk ]]; then
            continue
        fi

        pushd $dir

        # Loop over variables within suitable input directories
        for var in $(ls | xargs -n1 | cut -d. -f3 | sort | uniq); do
            echo Evaluating variable $var

            # tiled or non-tiled
            if ls $component.*.$var.tile1.nc; then
                is_tiled=1
            else
                is_tiled=""
            fi

            # for tiled files, use tile1 for the time range comparisons
            files=()
            for file in $(ls *.$var?(.tile1).nc); do
                date1=$(parse_date $(echo $file | cut -d. -f2 | cut -d- -f1))
                if [[ $date1 > $begin || $date1 == $begin ]] && [[ $date1 < $end ]]; then
                    files+=($file)
                fi
            done

            if (( ${#files[@]} != $expectedChunks )); then
                # Not sure what to do here- this is unexpected to us but possibly expected by user
                echo "WARNING: Skipping $var as unexpected number of chunks; expected '$expectedChunks', got '${#files[@]}'"
                continue
            fi

            # form new filename
            d1=$(echo ${files[0]} | cut -d. -f 2 | cut -d- -f 1)
            d2=$(echo ${files[-1]} | cut -d. -f 2 | cut -d- -f 2)

            # If processing subdirectories, add the subdir (e.g. "1deg") above the component level
            if [[ $use_subdirs ]]; then
                data_lineage_in_dir=$subdir/$component/$freq/$chunk
                data_lineage_out_dir=$subdir/$component/$freq/$outputChunk
                newdir=$outputDir/$subdir/$component/$freq/$outputChunk
            else
                data_lineage_in_dir=$component/$freq/$chunk
                data_lineage_out_dir=$component/$freq/$outputChunk
                newdir=$outputDir/$component/$freq/$outputChunk
            fi

            # create timeseries
            mkdir -p $newdir

            if [[ $is_tiled ]]; then
                for (( tile=1; tile <= 6; ++tile )); do
                    newfile=$component.$d1-$d2.$var.tile$tile.nc
                    tiled_files=$(echo ${files[@]} | sed -e s/tile1/tile$tile/g)
                    
                    if [ ! -z "${EPMT_DATA_LINEAGE+x}" ] && [ "$EPMT_DATA_LINEAGE" = "1" ]; then
                        for input_file in $tiled_files; do
                            hash_val=$(/home/Cole.Harvey/.conda/envs/bloom-filter-env/bin/python \
                            -m data_lineage.bloomfilter.HashGen $inputDir/$data_lineage_in_dir/$input_file)
                            export input_file_list="${input_file_list}$data_lineage_in_dir/$input_file $hash_val,"
                            echo "[DATA LINEAGE] Added $data_lineage_in_dir/$input_file to input list with hash_val: $hash_val"
                        done
                    fi

                    cdo --history -O mergetime $tiled_files $newdir/$newfile
                done
            else
                if [ ! -z "${EPMT_DATA_LINEAGE+x}" ] && [ "$EPMT_DATA_LINEAGE" = "1" ]; then
                    for input_file in ${files[@]}; do
                        hash_val=$(/home/Cole.Harvey/.conda/envs/bloom-filter-env/bin/python \
                        -m data_lineage.bloomfilter.HashGen $inputDir/$data_lineage_in_dir/$input_file)
                        export input_file_list="${input_file_list}$data_lineage_in_dir/$input_file $hash_val,"
                        echo "[DATA LINEAGE] Added $data_lineage_in_dir/$input_file to input list with hash_val: $hash_val"
                    done
                fi
                newfile=$component.$d1-$d2.$var.nc
                cdo --history -O mergetime ${files[@]} $newdir/$newfile
            fi

            if [ ! -z "${EPMT_DATA_LINEAGE+x}" ] && [ "$EPMT_DATA_LINEAGE" = "1" ]; then
                hash_val=$(/home/Cole.Harvey/.conda/envs/bloom-filter-env/bin/python \
                -m data_lineage.bloomfilter.HashGen $newdir/$newfile)
                export output_file_list="${output_file_list}$data_lineage_out_dir/$newfile $hash_val,"
                echo "[DATA LINEAGE] Added $data_lineage_out_dir/$newfile to output list with hash_val: $hash_val"
            fi

            did_something=1
        done
        popd
    done
}

source $(dirname ${BASH_SOURCE[0]})/../shared/shared.sh

shopt -s extglob

echo Arguments:
echo "    input dir: $inputDir"
echo "    output dir: $outputDir"
echo "    begin: $begin"
echo "    input chunk: $inputChunk"
echo "    output chunk: $outputChunk"
echo "    pp stop: $pp_stop"
echo "    component: $component"
echo "    components allowed to fail: ${fail_ok_components:=}"
echo "    use subdirs: ${use_subdirs:=}"
echo Utilities:
type cdo
type isodatetime

# Determine how many expected files to concatenate
inputChunkHrs=$(isodatetime --as-total=H $inputChunk | sed -e 's/\.0$//')
expectedChunks=$(( $(isodatetime --as-total=H $outputChunk | sed -e 's/\.0$//') / inputChunkHrs ))
availChunks=$(( $(isodatetime --as-total=H $begin $pp_stop --offset2=$inputChunk | sed -e 's/\.0$//') / inputChunkHrs ))

if ((availChunks >= expectedChunks))
then
    end=$(isodatetime $begin --offset $outputChunk)
else
    expectedChunks=$availChunks
    end=$(isodatetime $pp_stop --offset $inputChunk)
fi

if (( expectedChunks > 0 )); then
    echo NOTE: Expect to concatenate $expectedChunks subchunks
else
    echo "ERROR: Could not calculate number of expected chunks from input chunk '$inputChunk' and output chunk '$outputChunk'"
fi

# Verify input directory exists and is a directory
if [[ ! -d $inputDir ]]; then
    echo "Error: Input directory '${inputDir}' does not exist or isn't a directory"
    exit 1
fi

# Verify output directory exists and is a directory
if [[ ! -d $outputDir ]]; then
    echo "Error: Output directory '${outputDir}' does not exist or isn't a directory"
    exit 1
fi

# Setup PYTHONPATH and io lists for the data lineage tool
if [ ! -z "${EPMT_DATA_LINEAGE+x}" ] && [ "$EPMT_DATA_LINEAGE" = "1" ]; then
    export PYTHONPATH=$CYLC_SUITE_DEF_PATH:$PYTHONPATH
    export input_file_list=
    export output_file_list=
    echo "Set PYTHONPATH and created i/o lists"
fi

# remove trailing Z to allow string comparison later
begin=${begin%Z}
end=${end%Z}
echo "NOTE: Expect output date range to be [$begin, $end)"

cd $inputDir

# yuck
did_something=""

# If processing grid subdirectories, process each
if [[ $use_subdirs ]]; then
    for subdir in $(ls); do
        pushd $subdir/$component || continue
        process_timeseries
        popd
    done
# Otherwise, process the one location
else
    cd $component
    process_timeseries
fi


if [ ! -z "${EPMT_DATA_LINEAGE+x}" ] && [ "$EPMT_DATA_LINEAGE" = "1" ]; then

    epmt annotate EPMT_DATA_LINEAGE_IN_PATH="$inputDir/"
    echo -e "\n[COLE] annotated $inputDir to EPMT_DATA_LINEAGE_IN_PATH"

    epmt annotate EPMT_DATA_LINEAGE_OUT_PATH="$outputDir/"
    echo -e  "\n[COLE] annotated $outputDir to EPMT_DATA_LINEAGE_OUT_PATH"

    # Annotate to EPMT
    if [ -n "$input_file_list" ]; then
        compressed_bytes=$(/home/Cole.Harvey/.conda/envs/bloom-filter-env/bin/python \
        -m data_lineage.bloomfilter.StringCompression "${input_file_list}")
        epmt -v annotate EPMT_DATA_LINEAGE_IN="${compressed_bytes%*,}"
        echo "[DATA LINEAGE] Annotated input files to EPMT_LINEAGE_IN"
    fi

    if [ -n "$output_file_list" ]; then
        compressed_bytes=$(/home/Cole.Harvey/.conda/envs/bloom-filter-env/bin/python \
        -m data_lineage.bloomfilter.StringCompression "${output_file_list}")
        epmt -v annotate EPMT_DATA_LINEAGE_OUT="${compressed_bytes%*,}"
        echo "[DATA LINEAGE] Annotated output files to EPMT_LINEAGE_OUT"
    fi
fi

if [[ -n $did_something ]]; then
    echo Natural end of the timeseries generation
    exit 0
else
    if echo $fail_ok_components | grep "\b$component\b"; then
        echo No input files were found, but this is allowed
        exit 0
    else
        echo No input files were found
        exit 1
    fi
fi
